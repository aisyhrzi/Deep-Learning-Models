{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwdq4O5UFt6P5lgo2mf7aB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aisyhrzi/DeepLearning/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN - RELU"
      ],
      "metadata": {
        "id": "nDhLE8B1ya-o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-g-8VDcwA1o",
        "outputId": "08e6320a-9e2b-438b-f2d3-948c2bfa5229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 54s 28ms/step - loss: 0.1924 - accuracy: 0.9419\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 48s 25ms/step - loss: 0.0726 - accuracy: 0.9783\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.0531 - accuracy: 0.9837\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.0426 - accuracy: 0.9869\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 48s 25ms/step - loss: 0.0363 - accuracy: 0.9892\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0259 - accuracy: 0.9918\n",
            "Test accuracy: 0.9918000102043152\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4RwfJ2sZ0ZJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN -SOFTMAX\n"
      ],
      "metadata": {
        "id": "_8Kb7DBqzlc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='softmax', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsPLgMYdzn0C",
        "outputId": "57759b31-7a43-4118-827a-beadca74acfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.5586 - accuracy: 0.8181\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.1808 - accuracy: 0.9470\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.1321 - accuracy: 0.9609\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.1087 - accuracy: 0.9674\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0906 - accuracy: 0.9736\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0418 - accuracy: 0.9864\n",
            "Test accuracy: 0.9864000082015991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN- PRELU"
      ],
      "metadata": {
        "id": "69JG51ntz3Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='PReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qNquKoEz5LB",
        "outputId": "9e57c8ed-088a-4390-a11f-2fd0c8275918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 60s 31ms/step - loss: 0.2189 - accuracy: 0.9335\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0803 - accuracy: 0.9767\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.0580 - accuracy: 0.9829\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0452 - accuracy: 0.9865\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0389 - accuracy: 0.9881\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0286 - accuracy: 0.9904\n",
            "Test accuracy: 0.9904000163078308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN -ELU"
      ],
      "metadata": {
        "id": "kmlGqG5lz-JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='ELU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcOvna9m0DRn",
        "outputId": "02a4091d-645e-4c25-c820-10e2d8120b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 52s 27ms/step - loss: 0.2098 - accuracy: 0.9358\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0819 - accuracy: 0.9765\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0629 - accuracy: 0.9814\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0529 - accuracy: 0.9843\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0435 - accuracy: 0.9870\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0289 - accuracy: 0.9914\n",
            "Test accuracy: 0.9914000034332275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN - TANH"
      ],
      "metadata": {
        "id": "qbxwH8fP0Gg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='tanh', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaJvPaSY0IQS",
        "outputId": "0c10e4d2-aa21-4bf2-fc78-040b8d9a8e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2118 - accuracy: 0.9344\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 48s 25ms/step - loss: 0.0819 - accuracy: 0.9760\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0619 - accuracy: 0.9819\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0479 - accuracy: 0.9856\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0430 - accuracy: 0.9872\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0255 - accuracy: 0.9919\n",
            "Test accuracy: 0.9919000267982483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN-SIGMOID"
      ],
      "metadata": {
        "id": "75BcOoWs0K-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='sigmoid', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeGVYhJ90P3H",
        "outputId": "291c0aae-7542-4ae7-afe7-53e2973ef3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 52s 27ms/step - loss: 2.3019 - accuracy: 0.1110\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 2.3014 - accuracy: 0.1124\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 2.3014 - accuracy: 0.1124\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 2.3014 - accuracy: 0.1124\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 2.3014 - accuracy: 0.1124\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 2.3011 - accuracy: 0.1135\n",
            "Test accuracy: 0.11349999904632568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN -LeakyRelu\n"
      ],
      "metadata": {
        "id": "jrjXCt6q0fqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT7hHal-0pL0",
        "outputId": "cddaa52f-ff1f-45b3-cc3b-f6c2fafbbdb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 50s 26ms/step - loss: 0.2040 - accuracy: 0.9380\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0760 - accuracy: 0.9778\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0581 - accuracy: 0.9826\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0464 - accuracy: 0.9860\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0392 - accuracy: 0.9882\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0266 - accuracy: 0.9916\n",
            "Test accuracy: 0.991599977016449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN - ADAMAX + TANH"
      ],
      "metadata": {
        "id": "QvMMAQSgnwxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='tanh', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adamax',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI04LMOlnmXc",
        "outputId": "5e823865-0c60-44fd-e8b9-8732486b7053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.3415 - accuracy: 0.8955\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.1264 - accuracy: 0.9632\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.0926 - accuracy: 0.9729\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0783 - accuracy: 0.9767\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0677 - accuracy: 0.9798\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0359 - accuracy: 0.9883\n",
            "Test accuracy: 0.9883000254631042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN - TANH SGD"
      ],
      "metadata": {
        "id": "QcTSvwcrn3t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='tanh', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi0idrnOn68p",
        "outputId": "5eff7297-5914-4daa-fd8f-51e40c3deff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 48s 25ms/step - loss: 0.6872 - accuracy: 0.7890\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2539 - accuracy: 0.9261\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1873 - accuracy: 0.9443\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1499 - accuracy: 0.9560\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1329 - accuracy: 0.9594\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0705 - accuracy: 0.9779\n",
            "Test accuracy: 0.9779000282287598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN - TANH + RMSProp\n"
      ],
      "metadata": {
        "id": "b_m9TxtgoQtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='tanh', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='RMSProp',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1ZSBdNSnvTk",
        "outputId": "e68803d2-1414-4be3-fbed-d61f28eef396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 50s 26ms/step - loss: 0.1948 - accuracy: 0.9403\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.0755 - accuracy: 0.9778\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0583 - accuracy: 0.9831\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0531 - accuracy: 0.9851\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0507 - accuracy: 0.9857\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0372 - accuracy: 0.9886\n",
            "Test accuracy: 0.9886000156402588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN- TANH + ADAGRAD"
      ],
      "metadata": {
        "id": "8tn67DveoZFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='tanh', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='ADAGRAD',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-3kpMyuocMe",
        "outputId": "269097ec-73e1-41ef-8fc6-c09547617f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 50s 26ms/step - loss: 1.4913 - accuracy: 0.5520\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.6294 - accuracy: 0.8118\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 52s 27ms/step - loss: 0.4703 - accuracy: 0.8611\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.3983 - accuracy: 0.8829\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3550 - accuracy: 0.8971\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.2211 - accuracy: 0.9388\n",
            "Test accuracy: 0.9387999773025513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN - TANH + ALDADELTA"
      ],
      "metadata": {
        "id": "9j3cFsXDojQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='tanh', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='ADADELTA',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DglkGioWonEA",
        "outputId": "eedf1766-a5d9-4257-952a-e9bc2e74bcaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 57s 29ms/step - loss: 2.2688 - accuracy: 0.1707\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 2.1937 - accuracy: 0.3098\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 2.1108 - accuracy: 0.4195\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 2.0071 - accuracy: 0.4879\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 1.8776 - accuracy: 0.5371\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.7591 - accuracy: 0.7275\n",
            "Test accuracy: 0.7275000214576721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN - TANH + NADAM"
      ],
      "metadata": {
        "id": "wviCXG8FotrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='tanh', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='NADAM',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ex2i44ho1a6",
        "outputId": "c3eb5ccc-1bff-4e3b-de01-f1a63e0f4cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 58s 30ms/step - loss: 0.2053 - accuracy: 0.9382\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0797 - accuracy: 0.9763\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 59s 32ms/step - loss: 0.0633 - accuracy: 0.9818\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0513 - accuracy: 0.9844\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0427 - accuracy: 0.9867\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0332 - accuracy: 0.9905\n",
            "Test accuracy: 0.9904999732971191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN -tanh +SDG MOMENTUM"
      ],
      "metadata": {
        "id": "akcpTYAEL9I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='tanh', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmuvgVvVMGPW",
        "outputId": "964d4158-4a56-480e-c8a6-ab95f7ae4e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2652 - accuracy: 0.9151\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 48s 25ms/step - loss: 0.0982 - accuracy: 0.9711\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0749 - accuracy: 0.9775\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0600 - accuracy: 0.9822\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0503 - accuracy: 0.9847\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0291 - accuracy: 0.9899\n",
            "Test accuracy: 0.9898999929428101\n"
          ]
        }
      ]
    }
  ]
}