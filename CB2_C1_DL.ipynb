{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aisyhrzi/DeepLearning/blob/main/CB2_C1_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoder"
      ],
      "metadata": {
        "id": "lvan0YzN1_9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "print (x_train[1:3,1:3])\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "print (x_test[1:3,1:3])\n",
        "\n",
        "# print (x_train.shape)\n",
        "# print (x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiTikJTUNNvL",
        "outputId": "f088ea8f-d58d-4d7a-9e80-c9074660bac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.         0.         0.         0.00392157 0.         0.\n",
            "   0.         0.19215687 0.53333336 0.85882354 0.84705883 0.89411765\n",
            "   0.9254902  1.         1.         1.         1.         0.8509804\n",
            "   0.84313726 0.99607843 0.90588236 0.627451   0.1764706  0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.05490196\n",
            "   0.6901961  0.87058824 0.8784314  0.83137256 0.79607844 0.7764706\n",
            "   0.76862746 0.78431374 0.84313726 0.8        0.7921569  0.7882353\n",
            "   0.7882353  0.7882353  0.81960785 0.85490197 0.8784314  0.6431373\n",
            "   0.         0.         0.         0.        ]]\n",
            "\n",
            " [[0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.04705882 0.39215687 0.83137256 0.8039216\n",
            "   0.7254902  0.7019608  0.6784314  0.7294118  0.75686276 0.8666667\n",
            "   0.5568628  0.33333334 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.33333334 0.29803923 0.78039217\n",
            "   0.88235295 0.972549   1.         0.93333334 0.8862745  0.6156863\n",
            "   0.26666668 0.3137255  0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]]]\n",
            "[[[0.         0.         0.         0.         0.         0.\n",
            "   0.         0.03137255 0.47058824 0.81960785 0.8862745  0.96862745\n",
            "   0.92941177 1.         1.         1.         0.96862745 0.93333334\n",
            "   0.92156863 0.6745098  0.28235295 0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.5372549  0.9372549  0.9882353  0.9529412  0.91764706 0.8980392\n",
            "   0.93333334 0.95686275 0.9647059  0.9411765  0.9019608  0.9098039\n",
            "   0.9372549  0.972549   0.9843137  0.7607843  0.         0.\n",
            "   0.         0.         0.         0.        ]]\n",
            "\n",
            " [[0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.00784314 0.         0.76862746 1.\n",
            "   1.         1.         0.94509804 0.9843137  1.         0.9607843\n",
            "   1.         0.29803923 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.9529412  0.92941177\n",
            "   0.8509804  0.89411765 0.90588236 0.87058824 0.85490197 0.85882354\n",
            "   1.         0.45490196 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)\n",
        "\n",
        "class Autoencoder(Model):\n",
        "  def __init__(self, latent_dim, shape):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.shape = shape\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(latent_dim, activation='relu'),\n",
        "    ])\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Dense(tf.math.reduce_prod(shape), activation='sigmoid'),\n",
        "      layers.Reshape(shape)\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "\n",
        "shape = x_test.shape[1:]\n",
        "latent_dim = 64\n",
        "autoencoder = Autoencoder(latent_dim, shape)\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=10,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n",
        "\n",
        "encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
        "print(encoded_imgs)\n",
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
        "print(decoded_imgs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wQuvYdm2CwI",
        "outputId": "3df726ae-6b24-4cc4-dad8-bb3efea8886d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 11s 5ms/step - loss: 0.0239 - val_loss: 0.0134\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0117 - val_loss: 0.0106\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0101 - val_loss: 0.0098\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0095 - val_loss: 0.0095\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0092 - val_loss: 0.0092\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0091 - val_loss: 0.0091\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0090 - val_loss: 0.0092\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0089 - val_loss: 0.0091\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0088 - val_loss: 0.0089\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0088 - val_loss: 0.0089\n",
            "[[5.168745   2.1514525  3.2276251  ... 1.885102   2.444021   2.6210747 ]\n",
            " [1.9713292  2.4657254  0.         ... 3.133112   4.1949105  0.6466563 ]\n",
            " [4.9124     2.1473598  0.         ... 1.5863001  1.2076362  5.6366153 ]\n",
            " ...\n",
            " [3.0164914  2.7890673  0.         ... 0.99524057 0.9058191  1.7808266 ]\n",
            " [4.9196568  2.1684272  0.         ... 2.5173428  0.72082204 6.1998076 ]\n",
            " [3.107502   2.29032    1.6746919  ... 2.45963    1.6755481  2.1028206 ]]\n",
            "[[[3.39836419e-07 2.65441429e-07 3.29666864e-06 ... 1.37342533e-06\n",
            "   2.15806040e-07 2.48200394e-07]\n",
            "  [3.63691811e-07 2.54971360e-06 1.20281829e-05 ... 4.19689517e-04\n",
            "   4.20484808e-04 9.17930606e-07]\n",
            "  [2.36195069e-06 1.65271842e-06 1.66576353e-06 ... 1.45518454e-04\n",
            "   7.61718431e-04 5.00374939e-04]\n",
            "  ...\n",
            "  [1.35550770e-04 4.25180078e-05 6.27896079e-05 ... 1.12650014e-04\n",
            "   2.53588310e-04 2.17541528e-04]\n",
            "  [3.09263232e-05 1.24526368e-05 8.93766719e-06 ... 4.30288201e-05\n",
            "   2.67717569e-05 2.07694073e-04]\n",
            "  [7.62653656e-07 2.21987975e-05 5.19039859e-05 ... 1.55915288e-04\n",
            "   2.40770489e-04 1.82357144e-05]]\n",
            "\n",
            " [[1.08518101e-08 9.21787802e-09 4.45064245e-07 ... 5.38536469e-08\n",
            "   1.17963914e-08 4.68324979e-09]\n",
            "  [2.62890580e-08 6.87902457e-07 2.94876460e-04 ... 2.00099149e-03\n",
            "   6.42856758e-04 8.43658512e-08]\n",
            "  [1.52757352e-07 4.20917672e-07 1.88848964e-04 ... 4.94212518e-03\n",
            "   1.63311942e-03 1.88067468e-04]\n",
            "  ...\n",
            "  [1.69262217e-04 1.29570399e-04 5.67824231e-04 ... 6.18093414e-03\n",
            "   3.19522689e-04 1.38769188e-04]\n",
            "  [3.74641022e-05 9.21917526e-05 1.20408053e-03 ... 1.16419839e-02\n",
            "   1.89526138e-04 2.05528704e-04]\n",
            "  [4.98421230e-07 3.74231331e-06 6.14916091e-04 ... 7.82846007e-03\n",
            "   9.06214409e-05 9.21228002e-06]]\n",
            "\n",
            " [[9.52580770e-09 2.55284860e-08 2.10409610e-07 ... 2.47193611e-07\n",
            "   4.04102707e-09 7.68761854e-09]\n",
            "  [1.16045861e-07 1.48464707e-07 8.09156063e-06 ... 1.43196451e-06\n",
            "   1.60497552e-06 4.04278708e-08]\n",
            "  [1.09080098e-07 1.15325385e-08 5.14678433e-08 ... 1.32999567e-05\n",
            "   5.44186548e-07 4.00619456e-06]\n",
            "  ...\n",
            "  [5.13333725e-06 5.60078443e-05 9.93299182e-06 ... 7.00564300e-08\n",
            "   3.64687294e-05 9.71118861e-05]\n",
            "  [2.88682600e-06 6.32814499e-06 9.26470148e-06 ... 2.21658141e-07\n",
            "   1.02326499e-04 2.07625781e-04]\n",
            "  [6.94723383e-07 5.36231369e-07 2.88456467e-05 ... 1.15467890e-06\n",
            "   7.71207488e-06 5.08535686e-06]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[2.47989851e-06 5.15093734e-06 1.34732827e-05 ... 1.37117768e-05\n",
            "   2.86857858e-06 1.69707607e-06]\n",
            "  [9.10692597e-06 3.05041776e-05 3.40788974e-04 ... 5.97550825e-04\n",
            "   6.54773088e-04 2.52012615e-06]\n",
            "  [2.62718440e-05 2.33747742e-05 1.43904443e-04 ... 4.37571493e-04\n",
            "   5.72861580e-04 2.98143248e-03]\n",
            "  ...\n",
            "  [1.66971382e-04 1.53780289e-04 4.42508783e-04 ... 6.07324706e-04\n",
            "   9.90770292e-04 1.19168672e-03]\n",
            "  [4.22696976e-05 7.05207422e-05 4.99323651e-04 ... 6.60718477e-04\n",
            "   1.20833609e-03 6.39091479e-04]\n",
            "  [1.75092355e-05 9.06050191e-05 6.52785995e-04 ... 6.12863863e-04\n",
            "   6.67420449e-04 3.31049996e-05]]\n",
            "\n",
            " [[2.30598374e-07 4.77131493e-07 4.49444860e-06 ... 1.05063862e-06\n",
            "   1.51505134e-07 1.80405081e-07]\n",
            "  [1.45854017e-06 1.54996246e-06 2.83756272e-05 ... 9.41229155e-05\n",
            "   1.31672678e-05 8.98712017e-07]\n",
            "  [2.18918194e-06 2.83728703e-08 1.27449027e-07 ... 1.89619659e-05\n",
            "   4.12949674e-07 1.81034538e-05]\n",
            "  ...\n",
            "  [2.97876850e-05 7.60674493e-06 2.49583309e-06 ... 3.42343151e-06\n",
            "   2.42179274e-04 2.39587258e-04]\n",
            "  [1.26346531e-05 2.12006125e-05 1.70408948e-05 ... 9.54497318e-06\n",
            "   7.92805396e-04 8.71357915e-05]\n",
            "  [4.16728881e-06 1.38405073e-06 2.73809601e-05 ... 2.03647432e-04\n",
            "   9.26508292e-05 2.66869010e-05]]\n",
            "\n",
            " [[3.65627488e-06 6.70381269e-06 2.24307551e-05 ... 1.46317889e-05\n",
            "   1.72505770e-06 4.72434704e-06]\n",
            "  [4.89353579e-06 2.05402976e-05 1.55581598e-04 ... 1.31485111e-03\n",
            "   1.22977595e-03 7.75506578e-06]\n",
            "  [2.97087045e-05 2.50965441e-05 8.06935423e-05 ... 6.91463822e-04\n",
            "   1.68768247e-03 6.31509407e-04]\n",
            "  ...\n",
            "  [8.79920553e-04 5.00412891e-04 3.58865422e-04 ... 4.53632179e-04\n",
            "   1.58764690e-03 1.25839619e-03]\n",
            "  [4.14597453e-04 7.44537567e-04 6.31242234e-04 ... 2.32606078e-04\n",
            "   4.61828575e-04 1.02522608e-03]\n",
            "  [1.59892643e-05 1.82956690e-04 5.15450607e-04 ... 6.91357651e-04\n",
            "   7.14383670e-04 7.69698163e-05]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the dimensions of the input data\n",
        "input_dim = 784  # For MNIST dataset, each image is 28x28 pixels\n",
        "\n",
        "# Define the encoder model\n",
        "encoder_input = Input(shape=(input_dim,))\n",
        "encoder = Dense(128, activation='relu')(encoder_input)\n",
        "encoder_output = Dense(64, activation='relu')(encoder)\n",
        "\n",
        "# Define the decoder model\n",
        "decoder = Dense(128, activation='relu')(encoder_output)\n",
        "decoder_output = Dense(input_dim, activation='sigmoid')(decoder)\n",
        "\n",
        "# Combine the encoder and decoder into the autoencoder model\n",
        "autoencoder = Model(encoder_input, decoder_output)\n",
        "\n",
        "# Compile the autoencoder\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')\n",
        "\n",
        "# Load and preprocess the dataset (example: MNIST)\n",
        "(X_train, _), (X_test, _) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype('float32') / 255.\n",
        "X_test = X_test.astype('float32') / 255.\n",
        "X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
        "X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(X_train, X_train, epochs=10, batch_size=256, shuffle=True, validation_data=(X_test, X_test))\n",
        "\n",
        "# Once trained, you can use the autoencoder for reconstruction\n",
        "reconstructed_images = autoencoder.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp19f-0J29V2",
        "outputId": "419fc30a-0892-4a64-b3b3-472a7919b28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "235/235 [==============================] - 9s 32ms/step - loss: 0.2213 - val_loss: 0.1403\n",
            "Epoch 2/10\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.1241 - val_loss: 0.1102\n",
            "Epoch 3/10\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1060 - val_loss: 0.0998\n",
            "Epoch 4/10\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0982 - val_loss: 0.0949\n",
            "Epoch 5/10\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.0936 - val_loss: 0.0910\n",
            "Epoch 6/10\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.0905 - val_loss: 0.0885\n",
            "Epoch 7/10\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0882 - val_loss: 0.0865\n",
            "Epoch 8/10\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0864 - val_loss: 0.0846\n",
            "Epoch 9/10\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0849 - val_loss: 0.0834\n",
            "Epoch 10/10\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0836 - val_loss: 0.0821\n",
            "313/313 [==============================] - 1s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN"
      ],
      "metadata": {
        "id": "qyEotYW6XuZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko3Nd3llU8uc",
        "outputId": "fc869a8e-7efc-4e6b-f9a4-a20cadfef332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define and train the ANN model\n",
        "model = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Loading the dataset\n",
        "(X_train, y_train),(X_test, y_test)=fashion_mnist.load_data()\n",
        "\n",
        "# Normalizing the images\n",
        "X_train=X_train/255\n",
        "X_test=X_test/255\n",
        "\n",
        "# Reshaping the data\n",
        "X_train.shape\n",
        "X_train=X_train.reshape(-1,28*28)\n",
        "X_train.shape\n",
        "X_test=X_test.reshape(-1,28*28)\n",
        "X_test.shape\n",
        "\n",
        "# 1. Defining the model\n",
        "model=tf.keras.models.Sequential()\n",
        "\n",
        "# 2. Adding a first fully connected hidden layer\n",
        "model.add(tf.keras.layers.Dense(units=128, activation='relu',input_shape=(784,)))\n",
        "# number of units/neurons: 128\n",
        "# activation function: ReLU\n",
        "# input_shape: (784,)\n",
        "\n",
        "# 3. Adding second layer with dropout\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "# 4. Adding the output layer\n",
        "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
        "# units: number of classes (10 in Fashion MNIST dataset)\n",
        "# activation: softmax\n",
        "\n",
        "# 5. Compiling the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
        "# Optimizer: Adam\n",
        "# Loss: Sparse softmax (categorical) crossentropy\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 6. Training the model\n",
        "model.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "# 7. Model evaluation and prediction\n",
        "test_loss, test_accuracy=model.evaluate(X_test, y_test)\n",
        "print(\"Test accuracy: {}\".format(test_accuracy))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zikdRQOnVRFc",
        "outputId": "7a7a4cb7-960a-418a-8b53-d3fe02aae339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               100480    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 101770 (397.54 KB)\n",
            "Trainable params: 101770 (397.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5298 - sparse_categorical_accuracy: 0.8127\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4033 - sparse_categorical_accuracy: 0.8539\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3684 - sparse_categorical_accuracy: 0.8659\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3434 - sparse_categorical_accuracy: 0.8734\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3314 - sparse_categorical_accuracy: 0.8777\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3179 - sparse_categorical_accuracy: 0.8828\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3101 - sparse_categorical_accuracy: 0.8852\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2986 - sparse_categorical_accuracy: 0.8886\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2923 - sparse_categorical_accuracy: 0.8908\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2840 - sparse_categorical_accuracy: 0.8941\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3365 - sparse_categorical_accuracy: 0.8801\n",
            "Test accuracy: 0.8801000118255615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN"
      ],
      "metadata": {
        "id": "CzDpwtn-X-gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Define the architecture of the DNN\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(784,)),  # Input layer with 128 neurons and ReLU activation\n",
        "    Dense(64, activation='relu'),                       # Hidden layer with 64 neurons and ReLU activation\n",
        "    Dense(10, activation='softmax')                     # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "# Flatten the data\n",
        "x_train_flattened = x_train.reshape(-1, 28*28)\n",
        "x_test_flattened = x_test.reshape(-1, 28*28)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train_flattened, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test_flattened, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPmE76HXYAB_",
        "outputId": "e4137d1a-2c8b-42df-84ac-556663b5842a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2354 - accuracy: 0.9315\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0994 - accuracy: 0.9698\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0704 - accuracy: 0.9782\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0549 - accuracy: 0.9821\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0420 - accuracy: 0.9863\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9732\n",
            "Test accuracy: 0.9732000231742859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "Z68TysDjYK-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zejg8yb2YMZv",
        "outputId": "55fcefba-0749-4bb2-89b7-6fe1d97b0a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.2046 - accuracy: 0.9374\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0777 - accuracy: 0.9777\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0575 - accuracy: 0.9827\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0483 - accuracy: 0.9855\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0403 - accuracy: 0.9874\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0258 - accuracy: 0.9917\n",
            "Test accuracy: 0.9916999936103821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Convolution Models"
      ],
      "metadata": {
        "id": "3ApKAb2uvNtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='relu'),                         # Convolutional layer with 128 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NGLpuSKvQoW",
        "outputId": "92b63b2c-768e-4198-9cdb-2b29a439d37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 39s 20ms/step - loss: 0.2629 - accuracy: 0.9187\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0856 - accuracy: 0.9759\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0625 - accuracy: 0.9823\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0497 - accuracy: 0.9854\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0407 - accuracy: 0.9882\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0478 - accuracy: 0.9856\n",
            "Test accuracy: 0.9855999946594238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN"
      ],
      "metadata": {
        "id": "-PI6lYf_Yslv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "model = keras.Sequential()\n",
        "# Add an Embedding layer expecting input vocab of size 1000, and\n",
        "# output embedding dimension of size 64.\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# Add a LSTM layer with 128 internal units.\n",
        "model.add(layers.LSTM(128))\n",
        "\n",
        "# Add a Dense layer with 10 units.\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNPbO3lZYtwd",
        "outputId": "78be66a2-47a6-41ea-be59-c2480f601b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          64000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               98816     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 164106 (641.04 KB)\n",
            "Trainable params: 164106 (641.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
        "model.add(layers.GRU(256, return_sequences=True))\n",
        "\n",
        "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
        "model.add(layers.SimpleRNN(128))\n",
        "\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX9S86zusDYm",
        "outputId": "a039742b-0e5d-4682-8a39-21c017e12ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 64)          64000     \n",
            "                                                                 \n",
            " gru (GRU)                   (None, None, 256)         247296    \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 128)               49280     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 361866 (1.38 MB)\n",
            "Trainable params: 361866 (1.38 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the sequence length and number of features\n",
        "sequence_length = 10\n",
        "num_features = 1  # For simplicity, let's assume each element in the sequence is a single feature\n",
        "\n",
        "# Generate some sample sequential data\n",
        "X_train = np.random.randn(100, sequence_length, num_features)\n",
        "y_train = np.random.randint(0, 2, size=(100,))  # Binary classification task, random labels for demonstration\n",
        "\n",
        "# Define the RNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.SimpleRNN(32, input_shape=(sequence_length, num_features)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Once trained, you can use the model for predictions\n",
        "# For example:\n",
        "# predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKfkChtGtGH4",
        "outputId": "582c667f-30ea-48df-f60a-ec403730cc5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 2s 8ms/step - loss: 0.6963 - accuracy: 0.5100\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6676 - accuracy: 0.6200\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6501 - accuracy: 0.6300\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6384 - accuracy: 0.6000\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6289 - accuracy: 0.6100\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6235 - accuracy: 0.6300\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6178 - accuracy: 0.6700\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6157 - accuracy: 0.6500\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6126 - accuracy: 0.6300\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6099 - accuracy: 0.6300\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e4f3c83cd90>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN"
      ],
      "metadata": {
        "id": "JwNd4Qcz0Ylx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the dimensions of the latent space (input to the generator)\n",
        "latent_dim = 100\n",
        "\n",
        "# Define the generator model\n",
        "generator = Sequential([\n",
        "    Dense(128, input_dim=latent_dim, activation='relu'),\n",
        "    Dense(784, activation='sigmoid'),\n",
        "    Reshape((28, 28))\n",
        "])\n",
        "\n",
        "# Define the discriminator model\n",
        "discriminator = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the discriminator (binary crossentropy loss)\n",
        "discriminator.compile(optimizer=Adam(learning_rate=0.0002), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Freeze the discriminator during the combined model training\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Define the combined model (GAN)\n",
        "gan = Sequential([generator, discriminator])\n",
        "\n",
        "# Compile the GAN (binary crossentropy loss)\n",
        "gan.compile(optimizer=Adam(learning_rate=0.0002), loss='binary_crossentropy')\n",
        "\n",
        "# Function to generate random noise (latent points)\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    return np.random.randn(n_samples, latent_dim)\n",
        "\n",
        "# Function to generate fake images using the generator\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "    latent_points = generate_latent_points(latent_dim, n_samples)\n",
        "    return generator.predict(latent_points)\n",
        "\n",
        "# Function to train the GAN\n",
        "def train_gan(gan, generator, discriminator, latent_dim, n_epochs=100, batch_size=128):\n",
        "    half_batch = batch_size // 2\n",
        "    for epoch in range(n_epochs):\n",
        "        # Train the discriminator\n",
        "        real_images = np.random.randn(half_batch, 28, 28)\n",
        "        real_labels = np.ones((half_batch, 1))\n",
        "        fake_images = generate_fake_samples(generator, latent_dim, half_batch)\n",
        "        fake_labels = np.zeros((half_batch, 1))\n",
        "        discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
        "        discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
        "        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
        "        # Train the generator (via the combined GAN model)\n",
        "        latent_points = generate_latent_points(latent_dim, batch_size)\n",
        "        misleading_targets = np.ones((batch_size, 1))\n",
        "        gan_loss = gan.train_on_batch(latent_points, misleading_targets)\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {gan_loss}\")\n",
        "\n",
        "# Train the GAN\n",
        "train_gan(gan, generator, discriminator, latent_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IARFP0dh0XHi",
        "outputId": "7157ecc3-06d7-4623-f693-0c485418d609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 1/100, Discriminator Loss: 0.9757413268089294, Generator Loss: 0.8630276322364807\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 2/100, Discriminator Loss: 0.8138350546360016, Generator Loss: 1.2077594995498657\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 3/100, Discriminator Loss: 0.7855572402477264, Generator Loss: 1.606438159942627\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 4/100, Discriminator Loss: 0.7415286898612976, Generator Loss: 2.0155181884765625\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 5/100, Discriminator Loss: 0.7305880859494209, Generator Loss: 2.4154045581817627\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 6/100, Discriminator Loss: 0.6771730482578278, Generator Loss: 2.747612237930298\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 7/100, Discriminator Loss: 0.6634741984307766, Generator Loss: 3.031863212585449\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch 8/100, Discriminator Loss: 0.6569011360406876, Generator Loss: 3.2804172039031982\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 9/100, Discriminator Loss: 0.6302485298365355, Generator Loss: 3.491830587387085\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch 10/100, Discriminator Loss: 0.5600013472139835, Generator Loss: 3.6348178386688232\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch 11/100, Discriminator Loss: 0.6494760103523731, Generator Loss: 3.7945902347564697\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 12/100, Discriminator Loss: 0.6530876476317644, Generator Loss: 3.88459849357605\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 13/100, Discriminator Loss: 0.5376379266381264, Generator Loss: 3.983006000518799\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 14/100, Discriminator Loss: 0.622345307841897, Generator Loss: 4.036371231079102\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 15/100, Discriminator Loss: 0.5712632872164249, Generator Loss: 4.102425575256348\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 16/100, Discriminator Loss: 0.5231911540031433, Generator Loss: 4.116600036621094\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 17/100, Discriminator Loss: 0.46850907523185015, Generator Loss: 4.149664878845215\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 18/100, Discriminator Loss: 0.5989111308008432, Generator Loss: 4.157070159912109\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 19/100, Discriminator Loss: 0.49527259077876806, Generator Loss: 4.1621832847595215\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 20/100, Discriminator Loss: 0.4886405849829316, Generator Loss: 4.152390003204346\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 21/100, Discriminator Loss: 0.5143090207129717, Generator Loss: 4.160930633544922\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 22/100, Discriminator Loss: 0.517249240539968, Generator Loss: 4.1347503662109375\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 23/100, Discriminator Loss: 0.5383378770202398, Generator Loss: 4.102352142333984\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 24/100, Discriminator Loss: 0.4564539473503828, Generator Loss: 4.140873908996582\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 25/100, Discriminator Loss: 0.4784127678722143, Generator Loss: 4.097415924072266\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 26/100, Discriminator Loss: 0.4601468201726675, Generator Loss: 4.048398017883301\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 27/100, Discriminator Loss: 0.5102094523608685, Generator Loss: 4.0572614669799805\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 28/100, Discriminator Loss: 0.43784449994564056, Generator Loss: 4.032214641571045\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 29/100, Discriminator Loss: 0.4815988037735224, Generator Loss: 4.013622760772705\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 30/100, Discriminator Loss: 0.45017003268003464, Generator Loss: 4.0006327629089355\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 31/100, Discriminator Loss: 0.4151734299957752, Generator Loss: 4.0194807052612305\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 32/100, Discriminator Loss: 0.46660691499710083, Generator Loss: 3.9962410926818848\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 33/100, Discriminator Loss: 0.41800981387495995, Generator Loss: 3.941702365875244\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 34/100, Discriminator Loss: 0.44281194917857647, Generator Loss: 3.9498143196105957\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 35/100, Discriminator Loss: 0.4285560678690672, Generator Loss: 3.931495189666748\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 36/100, Discriminator Loss: 0.4415534008294344, Generator Loss: 3.906320571899414\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 37/100, Discriminator Loss: 0.44869086146354675, Generator Loss: 3.8653712272644043\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 38/100, Discriminator Loss: 0.45601249020546675, Generator Loss: 3.8474326133728027\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 39/100, Discriminator Loss: 0.38383893854916096, Generator Loss: 3.82672119140625\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 40/100, Discriminator Loss: 0.3856929987668991, Generator Loss: 3.8228323459625244\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 41/100, Discriminator Loss: 0.3809556383639574, Generator Loss: 3.8147497177124023\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 42/100, Discriminator Loss: 0.4530376186594367, Generator Loss: 3.7867584228515625\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 43/100, Discriminator Loss: 0.3792545599862933, Generator Loss: 3.759026050567627\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 44/100, Discriminator Loss: 0.3623574785888195, Generator Loss: 3.7798619270324707\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 45/100, Discriminator Loss: 0.3440571576356888, Generator Loss: 3.744999885559082\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 46/100, Discriminator Loss: 0.4016872141510248, Generator Loss: 3.747368574142456\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 47/100, Discriminator Loss: 0.3721812414005399, Generator Loss: 3.7834842205047607\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 48/100, Discriminator Loss: 0.33600868470966816, Generator Loss: 3.7166528701782227\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 49/100, Discriminator Loss: 0.30531252454966307, Generator Loss: 3.687803268432617\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 50/100, Discriminator Loss: 0.3876145286485553, Generator Loss: 3.7210919857025146\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 51/100, Discriminator Loss: 0.3777419487014413, Generator Loss: 3.7215514183044434\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 52/100, Discriminator Loss: 0.34204015508294106, Generator Loss: 3.688495635986328\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 53/100, Discriminator Loss: 0.40606644097715616, Generator Loss: 3.7107396125793457\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 54/100, Discriminator Loss: 0.3165096417069435, Generator Loss: 3.716157913208008\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 55/100, Discriminator Loss: 0.37853914499282837, Generator Loss: 3.749936103820801\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 56/100, Discriminator Loss: 0.35038221813738346, Generator Loss: 3.679701089859009\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 57/100, Discriminator Loss: 0.341961320489645, Generator Loss: 3.6679320335388184\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 58/100, Discriminator Loss: 0.30286514945328236, Generator Loss: 3.672800064086914\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch 59/100, Discriminator Loss: 0.3061750456690788, Generator Loss: 3.7108120918273926\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 60/100, Discriminator Loss: 0.2915391456335783, Generator Loss: 3.7098710536956787\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 61/100, Discriminator Loss: 0.3416128661483526, Generator Loss: 3.6914100646972656\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 62/100, Discriminator Loss: 0.2770033534616232, Generator Loss: 3.6696128845214844\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 63/100, Discriminator Loss: 0.2884912732988596, Generator Loss: 3.739440679550171\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 64/100, Discriminator Loss: 0.3087727874517441, Generator Loss: 3.74741792678833\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 65/100, Discriminator Loss: 0.2774932440370321, Generator Loss: 3.76776123046875\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 66/100, Discriminator Loss: 0.30908612068742514, Generator Loss: 3.791208267211914\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 67/100, Discriminator Loss: 0.32206854969263077, Generator Loss: 3.791954755783081\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 68/100, Discriminator Loss: 0.2874089339748025, Generator Loss: 3.760650157928467\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 69/100, Discriminator Loss: 0.3008708478882909, Generator Loss: 3.7513647079467773\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 70/100, Discriminator Loss: 0.2580044437199831, Generator Loss: 3.8244991302490234\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 71/100, Discriminator Loss: 0.2707118894904852, Generator Loss: 3.722940444946289\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 72/100, Discriminator Loss: 0.24238906614482403, Generator Loss: 3.806349277496338\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 73/100, Discriminator Loss: 0.2828187569975853, Generator Loss: 3.835538387298584\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 74/100, Discriminator Loss: 0.2471405752003193, Generator Loss: 3.7985761165618896\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 75/100, Discriminator Loss: 0.22035535611212254, Generator Loss: 3.7275872230529785\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 76/100, Discriminator Loss: 0.28448432218283415, Generator Loss: 3.8126564025878906\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 77/100, Discriminator Loss: 0.232591082341969, Generator Loss: 3.760758399963379\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 78/100, Discriminator Loss: 0.24757694266736507, Generator Loss: 3.7830677032470703\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 79/100, Discriminator Loss: 0.22016130201518536, Generator Loss: 3.843916416168213\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 80/100, Discriminator Loss: 0.2285857442766428, Generator Loss: 3.7994680404663086\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch 81/100, Discriminator Loss: 0.24120893143117428, Generator Loss: 3.797482967376709\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 82/100, Discriminator Loss: 0.2378076519817114, Generator Loss: 3.9184093475341797\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 83/100, Discriminator Loss: 0.26628320198506117, Generator Loss: 3.8975937366485596\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 84/100, Discriminator Loss: 0.24595190677791834, Generator Loss: 3.871826648712158\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 85/100, Discriminator Loss: 0.2028120458126068, Generator Loss: 3.751477003097534\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 86/100, Discriminator Loss: 0.20294681750237942, Generator Loss: 3.885305404663086\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Epoch 87/100, Discriminator Loss: 0.22835475951433182, Generator Loss: 3.915651559829712\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 88/100, Discriminator Loss: 0.2372918426990509, Generator Loss: 3.9115748405456543\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Epoch 89/100, Discriminator Loss: 0.2415522336959839, Generator Loss: 3.7972195148468018\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 90/100, Discriminator Loss: 0.2105665635317564, Generator Loss: 3.868063449859619\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 91/100, Discriminator Loss: 0.2183542400598526, Generator Loss: 4.034156799316406\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch 92/100, Discriminator Loss: 0.2302738567814231, Generator Loss: 3.9310214519500732\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch 93/100, Discriminator Loss: 0.24773996509611607, Generator Loss: 3.904756784439087\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 94/100, Discriminator Loss: 0.18149712309241295, Generator Loss: 3.9487316608428955\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 95/100, Discriminator Loss: 0.20651305466890335, Generator Loss: 3.9761672019958496\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 96/100, Discriminator Loss: 0.21209795773029327, Generator Loss: 3.9621636867523193\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 97/100, Discriminator Loss: 0.18291697185486555, Generator Loss: 3.8928918838500977\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch 98/100, Discriminator Loss: 0.19346335623413324, Generator Loss: 4.018774032592773\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 99/100, Discriminator Loss: 0.18623567838221788, Generator Loss: 4.000369071960449\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch 100/100, Discriminator Loss: 0.1986838160082698, Generator Loss: 4.107605934143066\n"
          ]
        }
      ]
    }
  ]
}